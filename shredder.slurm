#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=2
#SBATCH --time=0-12:00:00
#SBATCH --mem=6G
#SBATCH --output=MJ-histo-BDA_%a.out
#SBATCH --mail-user=thomas.j.aiman@vanderbilt.edu
#SBATCH --mail-type=ALL
#SBATCH --array=136-431:5%150

# the '#!/bin/bash' tells the system what shell to use when interpreting our code (we're using bash)
# SLURM reads the sbatch commands but unix ignores them becuase they are behind # symbols

#'array' means that this code is run by SLURM as a parallel for loop. Each iteration of the loop assigns 
#a new task to a new processor and assigns one of the array numbers to the variable SLURM_ARRAY_TASK_ID. 
#We can later pass SLURM_ARRAY_TASK_ID to MATLAB to control the slice number.
#
#-edit array to be whatever slice numbers you want to run. The %200 dictates how many of these tasks 
#can be handled simultaneously. If you have a limited amount of processors available for your 
#lab/company/group, you might not want to hog all the resources so other people can still do stuff.
#
#TO submit a job array with index values between 1 and 31
#with a step size of 2 (i.e. 1, 3, 5, 7, 9 ... 31)
# -SBATCH --array=1-31:2 
#
#-edit 'output' to be what you want your .out files to be. Or get rid of it entirely so you don't get 
#.out files. The %a references the array number that is running.
#
#-edit 'mail-user' to your email address. It will tell you when the job starts, finishes, and if it fails.
#
#-should we leave ntasks=1?

#------------------------------- User Inputs -----------------------------------------------------
#'export' creates an environment variable. This is necessary to pass information to MATLAB/Python later.
export PROJECT='nSMDA'
export SUBJECT='new_mj'
export MONKEY_NAME='mj'
export STAIN='BDA'
export PICKLE_PATH="/home/aimantj/histology_shredder/slice-relationships/$PROJECT-$MONKEY_NAME-histo-$STAIN.p"
export SLICE_NUM=$SLURM_ARRAY_TASK_ID

#to get around ACCRE file system limitations and improve speed, we are going to do all our work in a 
#temporary folder on the node that the job gets assigned to. Name this whatever you want, but it should be 
#unique because other people will also be making folders in /tmp/
export TMP_DIR="/tmp/aimantj/$MONKEY_NAME-$STAIN-slice-$SLICE_NUM"
#set STOR_DIR to be where you want your finished .tar.gz files to be put after we are done. We have to 
#move the files because the /tmp/ folder is erased periodically and it only exists on that compute node; 
#it isn't shared. There's no way for us to get back to it.
export STOR_DIR="/scratch/aimantj/$MONKEY_NAME-$STAIN"
#---------------------------------------------------------------------------------------------------

#make our temporary folder. The $ means that the system uses the value of the variable WORK_DIR. Without 
#it, our folder would literally end up being called 'WORK_DIR'.
mkdir -p $TMP_DIR

#'echo' prints text to the terminal for you to see on screen. In our case it will print to the .out file 
#generated by slurm. Not necessary but useful if you need to debug. 
echo "SLURM_JOBID: " $SLURM_JOBID
echo "SLURM_ARRAY_TASK_ID: " $SLURM_ARRAY_TASK_ID
echo "SLURM_ARRAY_JOB_ID: " $SLURM_ARRAY_JOB_ID

echo "PROJECT: " $PROJECT
echo "SUBJECT: " $SUBJECT
echo "MONKEY_NAME: " $MONKEY_NAME
echo "STAIN: " $STAIN
echo "PICKLE_PATH: " $PICKLE_PATH
echo "SLICE_NUM: " $SLICE_NUM
echo "TMP_DIR: " $TMP_DIR
echo "STOR_DIR: " $STOR_DIR


#this line uses LMOD which runs on ACCRE to handle different pieces of software. It essentially adds 
#Anaconda3 to our shell search path. I've got the appropriate modules installed in an Anaconda virtual #environment, so we must activate that first before running our python script.

module load Anaconda3
source activate shredenv
python ~/histology_shredder/python-shredder/accre_shredder_wrapper.py






